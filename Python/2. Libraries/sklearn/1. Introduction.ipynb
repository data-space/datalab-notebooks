{"cells":[{"cell_type":"markdown","source":["# Scikit-learn"],"metadata":{}},{"cell_type":"markdown","source":["![alt](http://www.scatter.com/images/DataLab_logo.jpg)"],"metadata":{}},{"cell_type":"markdown","source":["General documentation and examples for Scikit-learn are available at\n- http://scikit-learn.org/stable/"],"metadata":{}},{"cell_type":"markdown","source":["__Relevant Libraries:__\n- `Numpy` Adds Python support for large, multi-dimensional arrays and matrices, along with a large library of high-level mathematical functions to operate on these arrays.\n- `SciPy` A collection of mathematical algorithms and convenience functions built on the `Numpy` extension of Python. It adds significant power to the interactive Python session by providing the user with high-level commands and classes for manipulating and visualizing data.\n- `Pandas` Software library written for data manipulation and analysis in Python. Offers data structures and operations for manipulating numerical tables and time series.\n- `Scikit-learn` A Python module for machine learning built on top of `SciPy` and distributed under the 3-Clause BSD license."],"metadata":{}},{"cell_type":"markdown","source":["## Table of Contents\n##### 1. Preprocessing\n- Imputation, Scaling\n- Feature Extraction\n\n##### 2. Unsupervised Learning\n- Dimensionality Reduction\n- Clustering\n\n##### 3. Supervised Learning\n- Regression\n- Classification\n\n##### 4. Model Selection\n- Cross Validation\n- Grid Search\n\n##### 5. Pipelines"],"metadata":{}},{"cell_type":"markdown","source":["In this tutorial we will use the `Boston` dataset, and the `Iris` dataset, both included with `sklearn`"],"metadata":{}},{"cell_type":"code","source":["import numpy as np\nimport pandas as pd\n\nfrom sklearn.datasets import load_boston\nboston = load_boston()\n\nfrom sklearn.datasets import load_iris\niris = load_iris()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["print(boston.DESCR)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["print(iris.DESCR)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["Convert the `.data` numpy arrays into pandas DataFrames for readability"],"metadata":{}},{"cell_type":"code","source":["boston_df = pd.DataFrame(boston.data, columns=['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT'])\nboston_df['MEDV'] = boston.target\n\niris_df = pd.DataFrame(iris.data, columns=['Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width'])\niris_df['Species'] = iris.target"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["## 1. Preprocessing\nhttp://scikit-learn.org/stable/modules/preprocessing.html#preprocessing"],"metadata":{}},{"cell_type":"markdown","source":["The preprocessing _transformations_ of Scikit-learn are classes with two methods:\n1. `fit` which takes one or more columns of a numpy array or pandas dataframe and records some information about the data in those columns\n1. `transform` which takes one or more columns of an array or dataframe (possibly the same as above, possibly different) and then returns a __numpy array__ based on the the _fitted_ data and the columns to _transform_"],"metadata":{}},{"cell_type":"markdown","source":["#### Imputation\nImputation replaces missing values with values based on the valid values in the row or column."],"metadata":{}},{"cell_type":"code","source":["from sklearn.preprocessing import Imputer"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["X = np.array([[1,     np.nan], \n             [np.nan, 4], \n             [7,      6]\n            ])\nX"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["We must initialize the imputer object with specifications before we can fit and transfrom `X`. We will name it `imp`."],"metadata":{}},{"cell_type":"code","source":["imp = Imputer(missing_values='NaN', strategy='mean', axis=0)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["Above we have specified the imputer to replace `'NaN'` with the `mean` of each column (`axis=0`).\n\nBelow, note fitting does not change `X`, it instead calculates the mean of each column and stores them in our object, `imp`."],"metadata":{}},{"cell_type":"code","source":["imp.fit(X)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["imp.statistics_"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["The `transform` method creates, in this case, a new array with the missing values of `X` replaced with the mean of the column containing the missing value."],"metadata":{}},{"cell_type":"code","source":["X_new = imp.transform(X)\nX_new"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["Our data sets do not have any missing values. Below we remove some values from the first column of our iris dataset to illustrate imputation."],"metadata":{}},{"cell_type":"code","source":["iris_with_missing = iris_df[:].copy()\niris_with_missing.iloc[[3, 4, 12, 34, 47, 54, 59, 63, 105, 115], 0] = np.nan\niris_with_missing.head()"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["iris_imp = Imputer(missing_values='NaN', strategy='mean', axis=0, verbose=1)\niris_imp.fit(iris_with_missing)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["iris_imp"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["iris_with_missing"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["pd.DataFrame(iris_filled, columns = iris_df.columns)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["Note that our dataframe is now a numpy array - we have lost our column headings and indices"],"metadata":{}},{"cell_type":"markdown","source":["__Exercise:__ Try changing the parameters in the examples above:\n- strategy = `mean`, `median`, `most_frequent`\n- axis = `0` (replace based on column), `1` (replace based on row)"],"metadata":{}},{"cell_type":"code","source":["iris_imp = Imputer(missing_values='NaN', strategy='median', axis=0)\niris_imp.fit(iris_with_missing)\niris_imp.transform(iris_with_missing)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["#### Scaling\nSome machine learning methods assume that a column of data is in a specific range (for instance, 0 to 1) or that the data has a mean of 0 and a standard deviation of 1. Scalers transform the values of a dataframe to meet these assumptions. They are especially useful before applying distance-based models, where a variable with unusually large values can outweight the rest. \n\nOptions:\n- `MinMaxScaler` - Scales column data such that the minimum for each column is 0 and the maximum is 1\n- `MaxAbsScaler` - Scales column data such that the maximum absolute value of each column is 1\n- `StandardScaler` - Standardizes column data such that the mean of each column is 0 and the standard deviation of each is 1\n- `Normalizer` - Scales __row__ data such that each row is a vector of length 1"],"metadata":{}},{"cell_type":"code","source":["from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, StandardScaler, Normalizer"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["__Exercise__: Compare each of the scalers below on `X` as well as our `iris_df`"],"metadata":{}},{"cell_type":"code","source":["X = np.array([[ 1., -1.,  2.],\n              [ 100.,  0.,  0.],\n              [ 0.,  1., -1.]])\nX"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["scaler = MinMaxScaler()\n\nscaler.fit(X)\nscaler.transform(X)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["iris_df.head()"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["iris_measurements = iris_df.drop('Species', axis=1)\niris_measurements.describe()"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["scaler_iris = MinMaxScaler()\n\nscaler_iris.fit(iris_measurements)\niris_scaled = scaler_iris.transform(iris_measurements)\npd.DataFrame(iris_scaled, columns=['Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width']).describe()"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["pd.DataFrame(iris_scaled, columns=['Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width']).head()"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":["#### Feature Extraction\nFeature extraction transforms variables so that they are more friendly to machine learning. Some examples include:\n- Converting continuous variables to binary\n- Converting categorical variables to binary dummy variables\n- Converting raw sentences to word counts"],"metadata":{}},{"cell_type":"code","source":["from sklearn.preprocessing import Binarizer, LabelBinarizer, OneHotEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["With `Binarizer` values above a given threshold become 1, and values equal-to or below the threshold become 0. We will demonstrate on our original array, `X`."],"metadata":{}},{"cell_type":"code","source":["X"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":["Below, instead of applying both `.fit()` and `.transform()`, we can instead equivalently use `.fit_transform()`"],"metadata":{}},{"cell_type":"code","source":["binarizer = Binarizer(threshold=0)\n\nprint(binarizer.fit(X).transform(X))\nprint('')\nprint(binarizer.fit_transform(X))"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":["`LabelBinarizer`\n- Takes as input (to the `fit` method) a multi-class 1-dimensional feature (list of __strings__)\n- Return as output (by the `transform` method) one or more binary features (numpy array)\n\n`OneHotEncoder`\n- Takes as input (to the `fit` method) a multi-class 1-dimensional feature (of categorical __integers__)\n- Return as output (by the `transform` method) one or more binary features (numpy array)"],"metadata":{}},{"cell_type":"code","source":["lb = LabelBinarizer()\nlb.fit(['A', 'B', 'A', 'C'])"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["lb.transform(['A', 'B', 'A', 'C'])"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":["Above, `.fit()` determines that there are 3 unique values to be turned into binary columns.\n\nBelow, notice \"D\" is lost from our new vector because it was not part of our fit procedure."],"metadata":{}},{"cell_type":"code","source":["lb.transform(['A', 'C', 'F'])"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"markdown","source":["__Exercise:__ Create your own 1D array and apply `LabelBinarizer`."],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":["In our `iris` dataset, our target variable `Species` is encoded as follows:\n- Iris-Setosa (0)\n- Iris-Versicolour (1)\n- Iris-Virginica (2)\n\nUsing `OneHotEncoder`, we can convert this into three binary variables."],"metadata":{}},{"cell_type":"code","source":["iris_df['Species'].value_counts()"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["ohe = OneHotEncoder()\ntargets = ohe.fit_transform(iris_df['Species'].reshape(-1, 1)) # .reshape(-1, 1) removes index from iris_df['Species]\ntargets.toarray() # Converts sparse matrix to array"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"markdown","source":["`CountVectorizer`\n- Takes as input (to the `fit` method) an array or list of __strings__\n- Return as output (by the `transform` method) a sparse matrix of token counts, in alphabetical order of tokens. With default settings tokens are words, and the vectorizer removes punctuation and lowers the case."],"metadata":{}},{"cell_type":"code","source":["documents = ['This is the first document.',\n             'This is the second second document.',\n             'And the third one.',\n            ]"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["vectorizer = CountVectorizer()\nvectorizer.fit(documents)\nprint(vectorizer.get_feature_names()) # Columns\n\nmatrix = vectorizer.transform(documents)\nprint(matrix.toarray()) # Counts"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["pd.DataFrame(matrix.toarray(), columns = vectorizer.get_feature_names())"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"markdown","source":["__Exercise:__ Write and vectorize your own sentences."],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"markdown","source":["## 2. Unsupervised Learning"],"metadata":{}},{"cell_type":"markdown","source":["#### Dimensionality Reduction\nThe most common form of dimensionality reduction is `Principal Components Analysis`\n\nWikipedia: https://en.wikipedia.org/wiki/Principal_component_analysis\n> Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components."],"metadata":{}},{"cell_type":"code","source":["from sklearn.decomposition import PCA"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"markdown","source":["If we return to our `iris` dataset, some of our predictor variables are highly correlated. PCA can help with this."],"metadata":{}},{"cell_type":"code","source":["iris_measurements.corr()"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"markdown","source":["Below, `.fit_transform()` returns a numpy array, which we convert again to a pandas DataFrame for readability."],"metadata":{}},{"cell_type":"code","source":["iris_measurements.head()"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"code","source":["pca = PCA()\niris_pca = pca.fit_transform(iris_measurements) \niris_pca = pd.DataFrame(iris_pca, columns=['PC1', 'PC2', 'PC3', 'PC4'])\niris_pca.head()"],"metadata":{},"outputs":[],"execution_count":73},{"cell_type":"markdown","source":["Our 4 variables are now measured in terms of our 4 principal components. By default, we always return as many principal components as there are variables. Note our 4 principal components are completely uncorrelated."],"metadata":{}},{"cell_type":"code","source":["np.round(iris_pca.corr(),2)"],"metadata":{},"outputs":[],"execution_count":75},{"cell_type":"markdown","source":["The other benefit of `PCA` is that the principal components are sorted in decreasing order of importance. Below, we can see that our first principal component explains __92%__ of the variation in our data, our second component explains __5%__, and the other two explain even less."],"metadata":{}},{"cell_type":"code","source":["pca.explained_variance_ratio_"],"metadata":{},"outputs":[],"execution_count":77},{"cell_type":"markdown","source":["The __Reduction__ part of Dimensionality Reduction comes from the fact that we can limit the number of principal components we want to keep in the returned dataset. We'll initialize the object `pca` to just use the first two, which explain almost 98% of the variation."],"metadata":{}},{"cell_type":"code","source":["pca = PCA(n_components = 2)\niris_pca = pca.fit_transform(iris_measurements) \niris_pca = pd.DataFrame(iris_pca, columns=['PC1', 'PC2'])\niris_pca.head()"],"metadata":{},"outputs":[],"execution_count":79},{"cell_type":"markdown","source":["__Exercise:__ Use PCA on the `boston_X` dataset below _(target variable `MEDV` removed)_. How many principal components would you keep?"],"metadata":{}},{"cell_type":"code","source":["boston_X = boston_df.drop('MEDV', axis=1)\nboston_X.corr()"],"metadata":{},"outputs":[],"execution_count":81},{"cell_type":"code","source":["pca = PCA()\npca.fit(boston_X)\npca.explained_variance_ratio_.round(2)"],"metadata":{},"outputs":[],"execution_count":82},{"cell_type":"code","source":["pd.DataFrame(pca.components_[0].round(2).reshape(-1,13), columns=boston_X.columns)"],"metadata":{},"outputs":[],"execution_count":83},{"cell_type":"code","source":["pca = PCA(n_components = 2)\npca.fit_transform(boston_X)"],"metadata":{},"outputs":[],"execution_count":84},{"cell_type":"markdown","source":["#### Clustering\nClustering groups similar data points into sets. There are many variants. We will focus on `K-Means`.\n\nWikipedia: https://en.wikipedia.org/wiki/K-means_clustering\n> K-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster"],"metadata":{}},{"cell_type":"code","source":["from sklearn.cluster import KMeans"],"metadata":{},"outputs":[],"execution_count":86},{"cell_type":"markdown","source":["We will use our `iris_scaled` array from earlier. Because `KMeans` is distance based, it would be biased to separate clusters along features with different scales.\n\nFrom the documentation, we know these plants belong to three species. Lets try to create three clusters and see how they compare to the real groupings. As a reminder, the first 5 rows of our scaled array are printed below."],"metadata":{}},{"cell_type":"code","source":["iris_scaled[0:5]"],"metadata":{},"outputs":[],"execution_count":88},{"cell_type":"markdown","source":["Below, we first specify a `random_state`. There is some randomness involved in identifying clusters, so this enables reproducibility. The `.fit()` method divides the data into clusters, and calculates the centers of each cluster. The `.predict()` method then labels data points according to the cluster centers calculated earlier."],"metadata":{}},{"cell_type":"code","source":["km = KMeans(n_clusters=3, random_state=1)\nkm.fit(iris_scaled)\nclusters = km.predict(iris_scaled)"],"metadata":{},"outputs":[],"execution_count":90},{"cell_type":"code","source":["pd.crosstab(clusters, iris_df['Species'])"],"metadata":{},"outputs":[],"execution_count":91},{"cell_type":"markdown","source":["Not too bad! The cluster numbers are meaningless, but we can see that species 0 (column) is completely identified in a cluster, species 1 (column) is almost completely identified in a cluster, and species 2 is split between two clusters."],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":93},{"cell_type":"markdown","source":["## 3. Supervised Learning"],"metadata":{}},{"cell_type":"markdown","source":["The supervised learning _estimators_ of Scikit-learn are classes with two methods:\n1. `fit` which takes a numpy array or pandas dataframe of predictor variables as well as a 1D array or dataframe for the target variable\n1. `predict` which takes a numpy array or pandas dataframe with the same columns as the predictor variables and returns a __numpy array__ of estimates based on the the _fitted_ data\n\n_Important note: all data must be numeric (or transformed to numeric from categories or labels above)_"],"metadata":{}},{"cell_type":"markdown","source":["#### Regression\nPredicting a continuous-valued attribute. We will use our `Boston` dataset, to predict `MEDV` (Median Home Value) based on the other predictors. Let's first split our data into training and test sets, so we can evaluate how well it predicts for unseen data.\n\n_Note: `train_test_split` converts our data into numpy arrays_"],"metadata":{}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(boston_df.drop('MEDV', axis=1),\n                                                    boston_df['MEDV'],\n                                                    test_size = 0.33, random_state=1\n                                                   )\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape"],"metadata":{},"outputs":[],"execution_count":97},{"cell_type":"markdown","source":["In this example we will use a simple `LinearRegression` model, and look at `Mean Squared Error` and `R-squared` to assess our predictions. We `.fit()` our model to the training data, `.predict()` based on the test data, and score the true y-values against our predictions."],"metadata":{}},{"cell_type":"code","source":["from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\ny_pred = lm.predict(X_test)\n\nprint('MSE:', mean_squared_error(y_test, y_pred))\nprint('R2:', r2_score(y_test, y_pred))"],"metadata":{},"outputs":[],"execution_count":99},{"cell_type":"markdown","source":["__Exercise:__ Use a linear regression to predict `CRIM` (crime rate) based on all other predictors. Make sure to split the data into training and test sets."],"metadata":{}},{"cell_type":"code","source":["boston_df.head()"],"metadata":{},"outputs":[],"execution_count":101},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":102},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":103},{"cell_type":"markdown","source":["#### Classification\nIdentifying to which category an object belongs to. We will use our __scaled__ `Iris` dataset to predict species using a `LogisticRegression` and evaluate the `accuracy score` (percent correct classes). Again we will split our data into training and test sets, `.fit()` to the training set, `.predict()` based on the test set, and score those predictions against the true categories."],"metadata":{}},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split(iris_scaled,\n                                                    iris_df['Species'],\n                                                    test_size = 0.33, random_state=1\n                                                   )"],"metadata":{},"outputs":[],"execution_count":105},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nlogfit = LogisticRegression(multi_class='multinomial', solver='newton-cg')\nlogfit.fit(X_train, y_train)\n\ny_pred = logfit.predict(X_test)\nprint(pd.crosstab(y_pred, y_test))\n\nprint('')\nprint('accuracy:', accuracy_score(y_test, y_pred))"],"metadata":{},"outputs":[],"execution_count":106},{"cell_type":"markdown","source":["We can also extract the predicted probabilities for each class. The prediction above is based on the class with the highest probability. \n\nBelow, we print the first 5 rows of these predicted probabilities."],"metadata":{}},{"cell_type":"code","source":["y_probs = logfit.predict_proba(X_test)\ny_probs[0:5]"],"metadata":{},"outputs":[],"execution_count":108},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":109},{"cell_type":"markdown","source":["For a complete list of classification and regression models, see http://scikit-learn.org/stable/supervised_learning.html#supervised-learning"],"metadata":{}},{"cell_type":"markdown","source":["## 4. Model Selection"],"metadata":{}},{"cell_type":"markdown","source":["To avoid overfitting, earlier we split our data into training and test sets, and only scored the model based on the test set. While this is a good process for understanding our final model's predictive power, if we leverage the test data too early in our process we risk overfitting by model choice. \n\nFor this reason, the best process is to do all of our fitting, model selection, and parameter tuning on _only the training data_, and to not use the test data until we have chosen our model and parameters. Only then does our test data gives us a true score."],"metadata":{}},{"cell_type":"markdown","source":["#### Cross Validation\nCross validation is the process of dividing data into equal subsets, and using each subset for scoring a model fit on all other subsets.\nWikipedia: https://en.wikipedia.org/wiki/Cross-validation_(statistics)"],"metadata":{}},{"cell_type":"markdown","source":["![alt](https://upload.wikimedia.org/wikipedia/commons/1/1c/K-fold_cross_validation_EN.jpg)"],"metadata":{}},{"cell_type":"markdown","source":["In scikit-learn, `cross_val_score` implements this process for a variety of metrics \n- Documentation: http://scikit-learn.org/stable/modules/cross_validation.html\n- List of metrics: http://scikit-learn.org/stable/modules/model_evaluation.html"],"metadata":{}},{"cell_type":"markdown","source":["Below, we will use cross validation on our training data from the Iris classification problem earlier. By setting `cv=5`, we divide our data into 5 folds."],"metadata":{}},{"cell_type":"code","source":["from sklearn.model_selection import cross_val_score\n\nlogfit = LogisticRegression(multi_class='multinomial', solver='newton-cg')\nscores = cross_val_score(logfit, \n                         X_train, \n                         y_train, \n                         scoring='accuracy', \n                         cv=5)\n\nprint(scores)\nprint(scores.mean())"],"metadata":{},"outputs":[],"execution_count":117},{"cell_type":"markdown","source":["__Exercise:__ Pick a different metric and score the training data."],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":119},{"cell_type":"markdown","source":["#### Grid Search\nIf our model has parameters we can tune, `GridSearchCV` can help. It implements the cross-validation process above, but calculates scores for a grid of possible input parameters. We can then use the best scoring model to make predictions."],"metadata":{}},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier"],"metadata":{},"outputs":[],"execution_count":121},{"cell_type":"markdown","source":["For this example, we will use `KNeighborsClassifier`, which is a supervised learning method that labels data points according to the most common class among its  `K` closest data points. For small values of `K`, the model draws a jagged boundary that is more susceptible to noise but can also fit nonlinear patterns. For larger values of `K`, the boundary between classes is smoothed. We will use `GridSearchCV` to find the ideal value of `K`."],"metadata":{}},{"cell_type":"code","source":["parameters = {'n_neighbors': [1, 5, 7, 11, 13],\n              'weights': ['uniform', 'distance']\n             }\n\nknn = KNeighborsClassifier()\ngrid = GridSearchCV(estimator=knn,\n                    param_grid = parameters,\n                    scoring = 'accuracy',\n                    cv = 3\n                   )"],"metadata":{},"outputs":[],"execution_count":123},{"cell_type":"markdown","source":["Notes on our parameters above:\n- For `n_neighbors`, because we have 3 classes and want to avoid ties, we do not use even numbers or numbers divisible by 3\n- For weights, `'uniform'` gives all points equal weight, `'distance'` gives more weight to closer points\n\nBelow, we fit our grid object to the scaled training data."],"metadata":{}},{"cell_type":"code","source":["grid.fit(X_train, y_train)\n\nprint('Best Accuracy Score:', grid.best_score_)\nprint('Best Parameters:', grid.best_params_)"],"metadata":{},"outputs":[],"execution_count":125},{"cell_type":"markdown","source":["The best-scoring model fits to the __5__ nearest neighbors, and gives each neighbor an equal weight. We can also make predictions using the grid object - it uses the best model. This was our best model and best parameters above, we could now bring back the test data."],"metadata":{}},{"cell_type":"code","source":["y_pred = grid.predict(X_test)\nprint('Accuracy:', accuracy_score(y_test, y_pred))"],"metadata":{},"outputs":[],"execution_count":127},{"cell_type":"markdown","source":["__Exercise:__ Make some changes to the parameter grid, and change the scoring metric. Are there any changes to the best model?"],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":129},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":130},{"cell_type":"markdown","source":["## 5. Pipelines"],"metadata":{}},{"cell_type":"markdown","source":["There are a lot of steps above to transform data in the way we want. Luckily, we can combine all of our transformers and an estimator into a pipeline object to speed up the process. Let's return to our messy iris data from earlier (`iris_with_missing`), and split that into training and test data."],"metadata":{}},{"cell_type":"code","source":["from sklearn.pipeline import Pipeline"],"metadata":{},"outputs":[],"execution_count":133},{"cell_type":"code","source":["iris_with_missing.head()"],"metadata":{},"outputs":[],"execution_count":134},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split(iris_with_missing.drop('Species', axis=1),\n                                                    iris_with_missing['Species'],\n                                                    test_size = 0.33, random_state=1\n                                                   )"],"metadata":{},"outputs":[],"execution_count":135},{"cell_type":"markdown","source":["A `Pipeline` object takes as input a list of tuples where each tuple has: \n- A name (`string`)\n- A transformer object \n\nThe objects run in sequence, and the last item in a `Pipeline` is typically an estimator object. Below we use an `Imputer`, `MinMaxScaler`, `PCA`, and our `KNeighborsClassifier` estimator."],"metadata":{}},{"cell_type":"code","source":["Pipe = Pipeline([('imputer', Imputer(strategy='mean')),\n                 ('scaler' , MinMaxScaler()),\n                 ('pca'    , PCA()),\n                 ('knn'    , KNeighborsClassifier(weights='uniform'))\n                 ])"],"metadata":{},"outputs":[],"execution_count":137},{"cell_type":"markdown","source":["Notice we haven't done anything with our data yet. We will now fit the pipeline to the training data, use `GridSearchCV` to select the best parameters, and finally make predictions on the test data. The pipeline will transform the test data exactly in the way it transforms the training data."],"metadata":{}},{"cell_type":"markdown","source":["To use a grid search with a pipeline, we must identify the step name from above, followed by two underscores, followed by the parameter name"],"metadata":{}},{"cell_type":"code","source":["parameters = {'pca__n_components': [1, 2, 3, 4],\n              'knn__n_neighbors': [5, 7, 11]\n             }\n\ngrid = GridSearchCV(estimator=Pipe,\n                    param_grid = parameters,\n                    scoring = 'accuracy',\n                    cv = 3\n                   )"],"metadata":{},"outputs":[],"execution_count":140},{"cell_type":"code","source":["grid.fit(X_train, y_train)\n\nprint('Best Accuracy Score:', grid.best_score_)\nprint('Best Parameters:', grid.best_params_)"],"metadata":{},"outputs":[],"execution_count":141},{"cell_type":"code","source":["y_pred = grid.predict(X_test)\nprint('Accuracy:', accuracy_score(y_test, y_pred))"],"metadata":{},"outputs":[],"execution_count":142},{"cell_type":"markdown","source":["__Exercise:__ Create and fit a pipeline to the `Boston` dataset. The pipeline should include a `scaler`, `PCA`, and `LinearRegression`. Use `GridSearchCV` to determine the best number of PCA components to include (based on `Mean Squared Error`).\n\nWhat is the `Mean Squared Error` of final predictions on the test data?"],"metadata":{}},{"cell_type":"code","source":["boston_df.head()"],"metadata":{},"outputs":[],"execution_count":144},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":145},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":146},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":147},{"cell_type":"markdown","source":["__The End__"],"metadata":{}}],"metadata":{"name":"1. Introduction","notebookId":210807},"nbformat":4,"nbformat_minor":0}